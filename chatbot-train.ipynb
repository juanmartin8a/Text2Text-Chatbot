{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRK1H7BN1iSR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP6JvTvh1l_0"
      },
      "outputs": [],
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def load_data_pairs(filename):\n",
        "    data_pairs = []\n",
        "    converasations = load_doc(filename)\n",
        "    converasations = converasations.split('\\n')\n",
        "    for pairs in converasations:\n",
        "        pair = pairs.split(' +++$+++ ')\n",
        "        data_pairs.append(pair)\n",
        "    return data_pairs\n",
        "\n",
        "def vocab_frequency(data_pairs):\n",
        "    vocab = {}\n",
        "    for pair in data_pairs:\n",
        "        for conv_tile in pair:\n",
        "            for word in conv_tile.split():\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 1\n",
        "                else:\n",
        "                    vocab[word] += 1\n",
        "    return vocab\n",
        "\n",
        "def vocab(vocab_dict):\n",
        "    threshold = 2\n",
        "    vocab = []\n",
        "    for word, val in vocab_dict.items():\n",
        "        if val > threshold:\n",
        "            vocab.append(word)\n",
        "    return vocab\n",
        "\n",
        "def clean(data_pairs, vocab):\n",
        "  for pair in range(len(data_pairs)):\n",
        "    for i in range(len(data_pairs[pair])):\n",
        "      sentence = data_pairs[pair][i].split(' ')\n",
        "      #print(sentence)\n",
        "      for j in range(len(sentence)):\n",
        "        if sentence[j] not in vocab:\n",
        "          sentence[j] = ''\n",
        "      sentence = ' '.join(sentence)\n",
        "      data_pairs[pair][i] = sentence\n",
        "  return data_pairs\n",
        "\n",
        "def tokenize(words, conv_tile):\n",
        "    vocab = words\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(vocab)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(conv_tile)\n",
        "    tensor = pad_sequences(tensor, padding='post')\n",
        "    return tokenizer, tensor\n",
        "\n",
        "def dataset_for_training(words, data_pairs):\n",
        "    questions = []\n",
        "    replies = []\n",
        "    for pair in data_pairs:\n",
        "        questions.append(pair[0])\n",
        "        replies.append(pair[1])\n",
        "    input_tokenizer, input_tensor = tokenize(words, questions)\n",
        "    target_tokenizer, target_tensor = tokenize(words, replies)\n",
        "    return input_tensor, target_tensor, input_tokenizer, target_tokenizer\n",
        "\n",
        "def max_length(data_pairs):\n",
        "    conv_tiles = []\n",
        "    for pair in data_pairs:\n",
        "        for conv_tile in pair:\n",
        "            conv_tiles.appen(conv_tile)\n",
        "    return max(len(words) for words in conv_tiles.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf1Ftq3Z1q4c",
        "outputId": "5763e171-e543-4734-84e3-32be17c29d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "there are 167413 conversation pairs\n",
            "there are 20482 words in vocab\n",
            "[['<start> well i thought we would start with  if that is okay with you <end>', '<start> not the hacking and gagging and spitting part please <end>'], ['<start> not the hacking and gagging and spitting part please <end>', '<start> okay then how about we try out some french cuisine saturday night <end>'], ['<start> you are asking me out that is so cute that is your name again <end>', '<start> forget it <end>'], ['<start> no no it is my fault we did not have a proper introduction <end>', '<start> cameron <end>'], ['<start> gosh if only we could find kat a boyfriend <end>', '<start> let me see what i can do <end>'], ['<start>  ma  this is my head <end>', '<start> right see you are ready for the quiz <end>'], ['<start> that is because it is such a nice one <end>', '<start> forget french <end>'], ['<start> how is our little find the wench a date plan  <end>', '<start> well theres someone i think might be <end>'], ['<start> there <end>', '<start> where <end>'], ['<start> you have my word as a gentleman <end>', '<start> you are sweet <end>'], ['<start> how do you get your hair to look like that <end>', '<start>  deep conditioner every two days and i never ever use a  without the   <end>'], ['<start> sure have <end>', '<start> i really really really wanna go but i cannot not unless my sister goes <end>'], ['<start> i really really really wanna go but i cannot not unless my sister goes <end>', '<start> i am working on it but she does not seem to be going for him <end>'], ['<start> so that is the kind of guy she likes pretty ones <end>', '<start> who knows all i have ever heard her say is that she would dip before dating a guy that smokes <end>'], ['<start> hi <end>', '<start> looks like things worked out tonight huh <end>'], ['<start> you know chastity <end>', '<start> i believe we share an art instructor <end>'], ['<start> have fun tonight <end>', '<start> tons <end>'], ['<start> i looked for you back at the party but you always seemed to be occupied <end>', '<start> i was <end>'], ['<start> i was <end>', '<start> you never wanted to go out with me did you <end>'], ['<start> well no <end>', '<start> then that is all you had to say <end>']]\n",
            "117189 117189 50224 50224\n"
          ]
        }
      ],
      "source": [
        "data_pairs = load_data_pairs('conversations.txt')\n",
        "print('there are ' + str(len(data_pairs)) + ' conversation pairs')\n",
        "\n",
        "vocab_frequency = vocab_frequency(data_pairs)\n",
        "vocab = vocab(vocab_frequency)\n",
        "vocab_size = len(vocab) + 1\n",
        "print('there are ' + str(vocab_size) + ' words in vocab')\n",
        "\n",
        "data_pairs = clean(data_pairs, vocab)\n",
        "print(data_pairs[:20])\n",
        "\n",
        "input_tensor, target_tensor, input_tokenizer, target_tokenizer = dataset_for_training(vocab, data_pairs)\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
        "    input_tensor, target_tensor, test_size=0.3\n",
        ")\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
        "\n",
        "buffer_size = len(input_tensor_train)\n",
        "batch_size = 64\n",
        "steps_per_epoch = len(input_tensor_train)\n",
        "embedding_dim = 360\n",
        "units = 1535\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(buffer_size)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnpe8Pls12QN"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    #self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self, x, hidden): #x is the input\n",
        "    x = self.embedding(x)\n",
        "    #output, state = self.gru(x, initial_state=hidden)\n",
        "    output, forward_h, backward_h = self.gru(x, initial_state=hidden)\n",
        "    state = tf.concat([forward_h, backward_h], 1)\n",
        "    return output, state\n",
        "\n",
        "  def init_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_size, self.enc_units)) for i in range(2)]\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return attention_weights, context_vector\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform'))\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, dec_input, dec_hidden, enc_output):\n",
        "    attention_weights, context_vector = self.attention(dec_hidden, enc_output)\n",
        "    dec_input = self.embedding(dec_input)\n",
        "    dec_input = tf.concat([tf.expand_dims(context_vector, 1), dec_input], axis=-1)\n",
        "    output, forward_h, backward_h = self.gru(dec_input)\n",
        "    state = tf.concat([forward_h, backward_h], 1)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    dec_input = self.fc(output)\n",
        "\n",
        "    return dec_input, state\n",
        "\n",
        "encoder = Encoder(vocab_size, embedding_dim, units, batch_size)\n",
        "decoder = Decoder(vocab_size, embedding_dim, units, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5IHzAm_EwEa"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "def loss_function(real, pred):\n",
        "  #logical_not inverses the array values? and equal checks if array is eual to somthing\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss.dtype) #cast converts specific values to different types\n",
        "  loss *= mask\n",
        "  return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckky1XO5K1E0"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = '/content/drive/MyDrive/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "#checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCR5qyeQK3cg"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRRKtxzPPnUI",
        "outputId": "458773df-b962-4d64-e61d-b8b5c9d59045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.5889\n",
            "Epoch 1 Batch 100 Loss 2.2637\n",
            "Epoch 1 Batch 200 Loss 1.9607\n",
            "Epoch 1 Batch 300 Loss 2.0754\n",
            "Epoch 1 Batch 400 Loss 2.1382\n",
            "Epoch 1 Batch 500 Loss 1.8533\n",
            "Epoch 1 Batch 600 Loss 1.8045\n",
            "Epoch 1 Batch 700 Loss 2.0734\n",
            "Epoch 1 Batch 800 Loss 2.1117\n",
            "Epoch 1 Batch 900 Loss 1.5241\n",
            "Epoch 1 Batch 1000 Loss 1.9126\n",
            "Epoch 1 Batch 1100 Loss 2.1467\n",
            "Epoch 1 Batch 1200 Loss 1.9532\n",
            "Epoch 1 Batch 1300 Loss 1.6134\n",
            "Epoch 1 Batch 1400 Loss 1.6871\n",
            "Epoch 1 Batch 1500 Loss 2.0645\n",
            "Epoch 1 Batch 1600 Loss 1.8147\n",
            "Epoch 1 Batch 1700 Loss 1.6418\n",
            "Epoch 1 Batch 1800 Loss 1.4214\n",
            "Epoch 1 Loss 0.0299\n",
            "Time taken for 1 epoch 959.4093389511108 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.5764\n",
            "Epoch 2 Batch 100 Loss 1.4888\n",
            "Epoch 2 Batch 200 Loss 1.6988\n",
            "Epoch 2 Batch 300 Loss 1.6182\n",
            "Epoch 2 Batch 400 Loss 1.7910\n",
            "Epoch 2 Batch 500 Loss 1.7959\n",
            "Epoch 2 Batch 600 Loss 1.7635\n",
            "Epoch 2 Batch 700 Loss 1.8053\n",
            "Epoch 2 Batch 800 Loss 1.7149\n",
            "Epoch 2 Batch 900 Loss 1.9843\n",
            "Epoch 2 Batch 1000 Loss 1.8439\n",
            "Epoch 2 Batch 1100 Loss 2.1290\n",
            "Epoch 2 Batch 1200 Loss 1.7705\n",
            "Epoch 2 Batch 1300 Loss 1.8488\n",
            "Epoch 2 Batch 1400 Loss 1.7532\n",
            "Epoch 2 Batch 1500 Loss 1.7444\n",
            "Epoch 2 Batch 1600 Loss 1.6994\n",
            "Epoch 2 Batch 1700 Loss 1.7805\n",
            "Epoch 2 Batch 1800 Loss 1.6467\n",
            "Epoch 2 Loss 0.0272\n",
            "Time taken for 1 epoch 935.370197057724 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7604\n",
            "Epoch 3 Batch 100 Loss 1.8344\n",
            "Epoch 3 Batch 200 Loss 1.6274\n",
            "Epoch 3 Batch 300 Loss 1.7037\n",
            "Epoch 3 Batch 400 Loss 1.7262\n",
            "Epoch 3 Batch 500 Loss 1.2662\n",
            "Epoch 3 Batch 600 Loss 1.6581\n",
            "Epoch 3 Batch 700 Loss 1.5496\n",
            "Epoch 3 Batch 800 Loss 1.5979\n",
            "Epoch 3 Batch 900 Loss 1.6964\n",
            "Epoch 3 Batch 1000 Loss 1.5250\n",
            "Epoch 3 Batch 1100 Loss 1.6583\n",
            "Epoch 3 Batch 1200 Loss 1.7297\n",
            "Epoch 3 Batch 1300 Loss 1.4889\n",
            "Epoch 3 Batch 1400 Loss 1.7331\n",
            "Epoch 3 Batch 1500 Loss 1.5517\n",
            "Epoch 3 Batch 1600 Loss 1.6255\n",
            "Epoch 3 Batch 1700 Loss 1.6083\n",
            "Epoch 3 Batch 1800 Loss 1.6509\n",
            "Epoch 3 Loss 0.0259\n",
            "Time taken for 1 epoch 928.3393661975861 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5112\n",
            "Epoch 4 Batch 100 Loss 1.4495\n",
            "Epoch 4 Batch 200 Loss 1.4718\n",
            "Epoch 4 Batch 300 Loss 1.5114\n",
            "Epoch 4 Batch 400 Loss 1.6595\n",
            "Epoch 4 Batch 500 Loss 1.4007\n",
            "Epoch 4 Batch 600 Loss 1.6585\n",
            "Epoch 4 Batch 700 Loss 1.3921\n",
            "Epoch 4 Batch 800 Loss 1.8704\n",
            "Epoch 4 Batch 900 Loss 1.5746\n",
            "Epoch 4 Batch 1000 Loss 1.7484\n",
            "Epoch 4 Batch 1100 Loss 1.7739\n",
            "Epoch 4 Batch 1200 Loss 1.4560\n",
            "Epoch 4 Batch 1300 Loss 1.5981\n",
            "Epoch 4 Batch 1400 Loss 1.5452\n",
            "Epoch 4 Batch 1500 Loss 1.7662\n",
            "Epoch 4 Batch 1600 Loss 1.6326\n",
            "Epoch 4 Batch 1700 Loss 1.5006\n",
            "Epoch 4 Batch 1800 Loss 1.6731\n",
            "Epoch 4 Loss 0.0247\n",
            "Time taken for 1 epoch 929.4536061286926 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4908\n",
            "Epoch 5 Batch 100 Loss 1.4236\n",
            "Epoch 5 Batch 200 Loss 1.3657\n",
            "Epoch 5 Batch 300 Loss 1.4978\n",
            "Epoch 5 Batch 400 Loss 1.4017\n",
            "Epoch 5 Batch 500 Loss 1.3879\n",
            "Epoch 5 Batch 600 Loss 1.5976\n",
            "Epoch 5 Batch 700 Loss 1.6090\n",
            "Epoch 5 Batch 800 Loss 1.4783\n",
            "Epoch 5 Batch 900 Loss 1.5623\n",
            "Epoch 5 Batch 1000 Loss 1.5673\n",
            "Epoch 5 Batch 1100 Loss 1.6747\n",
            "Epoch 5 Batch 1200 Loss 1.4262\n",
            "Epoch 5 Batch 1300 Loss 1.6985\n",
            "Epoch 5 Batch 1400 Loss 1.4396\n",
            "Epoch 5 Batch 1500 Loss 1.2603\n",
            "Epoch 5 Batch 1600 Loss 1.3270\n",
            "Epoch 5 Batch 1700 Loss 1.2420\n",
            "Epoch 5 Batch 1800 Loss 1.2806\n",
            "Epoch 5 Loss 0.0231\n",
            "Time taken for 1 epoch 928.6848120689392 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.2513\n",
            "Epoch 6 Batch 100 Loss 1.1000\n",
            "Epoch 6 Batch 200 Loss 1.1279\n",
            "Epoch 6 Batch 300 Loss 1.4532\n",
            "Epoch 6 Batch 400 Loss 1.2156\n",
            "Epoch 6 Batch 500 Loss 1.4605\n",
            "Epoch 6 Batch 600 Loss 1.4379\n",
            "Epoch 6 Batch 700 Loss 1.1740\n",
            "Epoch 6 Batch 800 Loss 1.5242\n",
            "Epoch 6 Batch 900 Loss 1.4485\n",
            "Epoch 6 Batch 1000 Loss 1.2021\n",
            "Epoch 6 Batch 1100 Loss 1.4468\n",
            "Epoch 6 Batch 1200 Loss 1.3208\n",
            "Epoch 6 Batch 1300 Loss 1.2340\n",
            "Epoch 6 Batch 1400 Loss 1.2398\n",
            "Epoch 6 Batch 1500 Loss 1.3816\n",
            "Epoch 6 Batch 1600 Loss 1.3372\n",
            "Epoch 6 Batch 1700 Loss 1.5341\n",
            "Epoch 6 Batch 1800 Loss 1.2231\n",
            "Epoch 6 Loss 0.0208\n",
            "Time taken for 1 epoch 933.0394794940948 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2226\n",
            "Epoch 7 Batch 100 Loss 0.9579\n",
            "Epoch 7 Batch 200 Loss 1.0454\n",
            "Epoch 7 Batch 300 Loss 1.0079\n",
            "Epoch 7 Batch 400 Loss 1.1038\n",
            "Epoch 7 Batch 500 Loss 1.1015\n",
            "Epoch 7 Batch 600 Loss 1.2356\n",
            "Epoch 7 Batch 700 Loss 1.1476\n",
            "Epoch 7 Batch 800 Loss 1.2350\n",
            "Epoch 7 Batch 900 Loss 1.1365\n",
            "Epoch 7 Batch 1000 Loss 1.2969\n",
            "Epoch 7 Batch 1100 Loss 1.3465\n",
            "Epoch 7 Batch 1200 Loss 1.0641\n",
            "Epoch 7 Batch 1300 Loss 1.1279\n",
            "Epoch 7 Batch 1400 Loss 1.0709\n",
            "Epoch 7 Batch 1500 Loss 1.1286\n",
            "Epoch 7 Batch 1600 Loss 1.0316\n",
            "Epoch 7 Batch 1700 Loss 1.2049\n",
            "Epoch 7 Batch 1800 Loss 1.1725\n",
            "Epoch 7 Loss 0.0178\n",
            "Time taken for 1 epoch 934.7919442653656 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.8051\n",
            "Epoch 8 Batch 100 Loss 0.9131\n",
            "Epoch 8 Batch 200 Loss 0.9190\n",
            "Epoch 8 Batch 300 Loss 0.8502\n",
            "Epoch 8 Batch 400 Loss 0.8957\n",
            "Epoch 8 Batch 500 Loss 1.0007\n",
            "Epoch 8 Batch 600 Loss 1.0187\n",
            "Epoch 8 Batch 700 Loss 1.0366\n",
            "Epoch 8 Batch 800 Loss 0.8663\n",
            "Epoch 8 Batch 900 Loss 1.0751\n",
            "Epoch 8 Batch 1000 Loss 0.8908\n",
            "Epoch 8 Batch 1100 Loss 0.8791\n",
            "Epoch 8 Batch 1200 Loss 1.0224\n",
            "Epoch 8 Batch 1300 Loss 0.8692\n",
            "Epoch 8 Batch 1400 Loss 1.0148\n",
            "Epoch 8 Batch 1500 Loss 0.9567\n",
            "Epoch 8 Batch 1600 Loss 0.8810\n",
            "Epoch 8 Batch 1700 Loss 1.0193\n",
            "Epoch 8 Batch 1800 Loss 0.9362\n",
            "Epoch 8 Loss 0.0145\n",
            "Time taken for 1 epoch 932.5199463367462 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.6277\n",
            "Epoch 9 Batch 100 Loss 0.6286\n",
            "Epoch 9 Batch 200 Loss 0.6126\n",
            "Epoch 9 Batch 300 Loss 0.6841\n",
            "Epoch 9 Batch 400 Loss 0.6983\n",
            "Epoch 9 Batch 500 Loss 0.6395\n",
            "Epoch 9 Batch 600 Loss 0.6346\n",
            "Epoch 9 Batch 700 Loss 0.7378\n",
            "Epoch 9 Batch 800 Loss 0.7156\n",
            "Epoch 9 Batch 900 Loss 0.7263\n",
            "Epoch 9 Batch 1000 Loss 0.6789\n",
            "Epoch 9 Batch 1100 Loss 0.6260\n",
            "Epoch 9 Batch 1200 Loss 0.6017\n",
            "Epoch 9 Batch 1300 Loss 0.7528\n",
            "Epoch 9 Batch 1400 Loss 0.8016\n",
            "Epoch 9 Batch 1500 Loss 0.7927\n",
            "Epoch 9 Batch 1600 Loss 0.7050\n",
            "Epoch 9 Batch 1700 Loss 0.7224\n",
            "Epoch 9 Batch 1800 Loss 0.8012\n",
            "Epoch 9 Loss 0.0113\n",
            "Time taken for 1 epoch 936.9459867477417 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.4013\n",
            "Epoch 10 Batch 100 Loss 0.5350\n",
            "Epoch 10 Batch 200 Loss 0.5052\n",
            "Epoch 10 Batch 300 Loss 0.4274\n",
            "Epoch 10 Batch 400 Loss 0.4810\n",
            "Epoch 10 Batch 500 Loss 0.4798\n",
            "Epoch 10 Batch 600 Loss 0.5828\n",
            "Epoch 10 Batch 700 Loss 0.5478\n",
            "Epoch 10 Batch 800 Loss 0.5194\n",
            "Epoch 10 Batch 900 Loss 0.5447\n",
            "Epoch 10 Batch 1000 Loss 0.5878\n",
            "Epoch 10 Batch 1100 Loss 0.6314\n",
            "Epoch 10 Batch 1200 Loss 0.5452\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.init_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  #if (epoch + 1) % 2 == 0:\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / (steps_per_epoch)))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7nkIpLn3w-Z"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q60EBpAk3Z-P"
      },
      "outputs": [],
      "source": [
        "def clean_sentence(sentence):\n",
        "    table = str.maketrans('','',string.punctuation)\n",
        "    sentence = sentence\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
        "    sentence = re.sub(r\"'til\", \"until\", sentence)\n",
        "    sentence = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", sentence)\n",
        "    words = sentence.split()\n",
        "    words = [word.lower() for word in words]\n",
        "    words = [word.translate(table) for word in words]\n",
        "    words = [word for word in words if(word.isalpha())]\n",
        "    sentence = '<start> ' + ' '.join(words) + ' <end>'\n",
        "    sentence_splitted = sentence.split()\n",
        "    for i, word in enumerate(sentence_splitted):\n",
        "      if word not in vocab:\n",
        "        sentence_splitted[i] = ''\n",
        "    sentence_splitted = ' '.join(sentence_splitted)\n",
        "    sentence = sentence_splitted\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD6VOb993fO5"
      },
      "outputs": [],
      "source": [
        "max_length = 22\n",
        "def evaluate(sentence):\n",
        "  sentence = clean_sentence(sentence)\n",
        "\n",
        "  inputs = [input_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length):\n",
        "    predictions, dec_hidden = decoder(dec_input, dec_hidden,  enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += target_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if target_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XsiZBK-3gmQ",
        "outputId": "bbd3b37f-1d6f-467a-8c00-013c4fbe1ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: <start> you might need more training <end>\n",
            "Reply: i am not i need to be a writer i am eccentric <end> \n"
          ]
        }
      ],
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Reply: {}'.format(result))\n",
        "\n",
        "translate('you might need more training')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
